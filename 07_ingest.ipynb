{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ingest.ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\r\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\r\n",
    "\r\n",
    "import re\r\n",
    "import pymongo\r\n",
    "import json\r\n",
    "import sc2reader\r\n",
    "import errno\r\n",
    "import jsonschema\r\n",
    "import os\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from typing import *\r\n",
    "from pathlib import Path\r\n",
    "from pprint import pprint\r\n",
    "from jsonschema import validate\r\n",
    "from dataclasses import dataclass, astuple, asdict, field\r\n",
    "\r\n",
    "from sc2reader.engine.plugins import APMTracker\r\n",
    "\r\n",
    "from sc_training.ingest import *\r\n",
    "sc2reader.engine.register_plugin(APMTracker())\r\n",
    "sc2reader.engine.register_plugin(CtrlGroupTracker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \r\n",
    "\r\n",
    "test_data_path = (Path(Path.cwd()/'test_replays') \r\n",
    "                  if Path('test_replays').exists() \r\n",
    "                  else Path('../../test_replays'))\r\n",
    "\r\n",
    "test_batch_path = (test_data_path / \"TestProfilerBatch\")\r\n",
    "\r\n",
    "assert test_data_path.exists()\r\n",
    "assert test_data_path.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - The main ingest module "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\r\n",
    "\r\n",
    "This section defines the ingest process' `inventory_replays` and `get_replay_indicators` functions following the design proposed in the Ingestion Sequence Diagram (see Section 1 - Data Ingestion and Clustering Process). It exports these functions into the ingest module of the ingest sub-package.\r\n",
    "\r\n",
    "### Exportable Members\r\n",
    "- `inventory_replays`\r\n",
    "- `get_replay_indicators`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventoring and Storing `replays` collection\n",
    "\n",
    "To inventory the replays in a replay batch I can use the `get_replay_info` defined in the `summarise_rpl` module (see Section 1.1).\n",
    "\n",
    "In the following code, I load a batch of replays and I print the summary of the first two replays using the `get_replay_info` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path:                   c:\\Users\\david\\Documents\\phdcode\\sc_training\\test_replays\\TestProfilerBatch\\16-Bit LE (2).SC2Replay \n",
      "File name:                   16-Bit LE (2).SC2Replay \n",
      "Date (datetime.datetime):    2021-06-04 03:49:10 \n",
      "Duration (seconds):          707 \n",
      "Game type:                   1v1 \n",
      "Game release:                5.0.7.84643 \n",
      "Map:                         16-Bit LE \n",
      "Game category:               Private \n",
      "winner:                      1 \n",
      "players:                     [(1, 'HDEspino', 'Terran', 'Win'), (2, 'A.I. 1 (Harder)', 'Zerg', 'Loss')] \n",
      "\n",
      "<class 'sc_training.ingest.summarise_rpl.Replay_data'>\n",
      "File path:                   c:\\Users\\david\\Documents\\phdcode\\sc_training\\test_replays\\TestProfilerBatch\\16-Bit LE (3).SC2Replay \n",
      "File name:                   16-Bit LE (3).SC2Replay \n",
      "Date (datetime.datetime):    2021-06-08 02:04:37 \n",
      "Duration (seconds):          384 \n",
      "Game type:                   1v1 \n",
      "Game release:                5.0.7.84643 \n",
      "Map:                         16-Bit LE \n",
      "Game category:               Private \n",
      "winner:                      1 \n",
      "players:                     [(1, 'HDEspino', 'Protoss', 'Win'), (2, 'A.I. 1 (Easy)', 'Protoss', 'Loss')] \n",
      "\n",
      "<class 'sc_training.ingest.summarise_rpl.Replay_data'>\n"
     ]
    }
   ],
   "source": [
    "replay_batch = sc2reader.load_replays(str(test_batch_path))\r\n",
    "\r\n",
    "for i, rpl in enumerate(replay_batch):\r\n",
    "    print(get_replay_info(rpl))\r\n",
    "    print(type(get_replay_info(rpl)))\r\n",
    "    if i == 1:\r\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing inventory in Database\n",
    "\n",
    "I can store the `Replay_data` objects returned by `get_replay_info` in a document-based database to illustrate how this function could store an inventory or replay information in such a format. Storing this inventory means that other processes could access it to navigate the information built by the ingest process.\n",
    "\n",
    "In the case of `sc_training`, I use a `config.json` file stored in the working project's data folder to set up a MongoDB local client and, through it, a database. The following code loads this file and defines the loading procedure for when this module is imported. \n",
    "\n",
    "I will divide this loading process into three steps. First, I locate and load the `config` file. This location process allows users to create a custom `config.json` in a data directory in their projects. That file can be used to customise the name of the database, the port address and number for the MongoDB client, and the location of the replays the user wants to process. \n",
    "\n",
    "Apart from this option, the code also defines how the module can default to a `config` file stored in the library's data folder if the user fails to provide this file. This default file allows the system to attempt to function based on the assumption that the users are using a Windows computer and have a traditional installation of StarCraft II and MongoDB. Of course, this default set-up would not be expected to work in all cases, but it provides an option and a sample of the `config` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the file is located, I also define a procedure to ensure it contains the necessary information to connect with the MongoDB client and access the appropriate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\r\n",
    "@dataclass\r\n",
    "class Config_settings:\r\n",
    "    port_address: str\r\n",
    "    port_number: int\r\n",
    "    db_name: str\r\n",
    "    replay_path: str\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        headers = [\"Port Address: \",\"Port Number: \",\r\n",
    "                  \"DB Name: \",\"Replays file: \"]\r\n",
    "        strings = [f'{h:<15}{att:>40}\\n' for h, att\r\n",
    "                   in zip(headers, astuple(self))]\r\n",
    "        return ''.join(strings)\r\n",
    "        \r\n",
    "Config_schema = {\r\n",
    "    \"type\": \"object\",\r\n",
    "    \"properties\":{\r\n",
    "        \"DB_NAME\": {\"type\":\"string\"},\r\n",
    "        \"PORT_ADDRESS\":  {\"type\":\"string\"},\r\n",
    "        \"PORT_NUMBER\": {\"type\":\"number\"},\r\n",
    "        \"REPLAY_PATH\": {\"type\":\"string\"}\r\n",
    "    }\r\n",
    "}\r\n",
    "\r\n",
    "def validate_config_file(file: Path, schema: Dict[str, Any]) -> bool:\r\n",
    "    try:\r\n",
    "        validate(file, schema)\r\n",
    "    except jsonschema.exceptions.ValidationError as err:\r\n",
    "        print(err)\r\n",
    "        print(\"config.json does not conform to the required specifications\")\r\n",
    "        raise err\r\n",
    "    except jsonschema.exceptions.SchemaError as err:\r\n",
    "        print(err)\r\n",
    "        print(\"The Config_schema is invalid\")\r\n",
    "        raise err\r\n",
    "    \r\n",
    "    return True\r\n",
    "\r\n",
    "def open_config_file(config_file: Path) -> dict[str, Any]:\r\n",
    "    try:\r\n",
    "        if not config_file.exists():\r\n",
    "            raise FileNotFoundError\r\n",
    "        \r\n",
    "        validate_config_file(json.load(config_file.open()), Config_schema)\r\n",
    "        \r\n",
    "        with config_file.open('r') as cf:\r\n",
    "            return json.load(cf)\r\n",
    "\r\n",
    "    except FileNotFoundError as err:\r\n",
    "        print('config.json not found')\r\n",
    "        raise err\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of those check and load procedures are contained within the `load_configurations` function. The following sample code shows the use of this fucntion to set up a test data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def load_configurations() -> Config_settings:\r\n",
    "    \"\"\"Loads the project's\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    config_file = (Path(Path.cwd()/'data/config.json') \r\n",
    "               if Path(Path.cwd()/'data/config.json').exists()\r\n",
    "               else Path(Path(__file__)/'../../../data/config.json'))\r\n",
    "\r\n",
    "    config_dict = open_config_file(config_file)\r\n",
    "    return Config_settings(\r\n",
    "        config_dict['PORT_ADDRESS'],\r\n",
    "        config_dict['PORT_NUMBER'],\r\n",
    "        config_dict['DB_NAME'],\r\n",
    "        config_dict['REPLAY_PATH']\r\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_settings = load_configurations()\r\n",
    "mongo_client = pymongo.MongoClient(db_settings.port_address, \r\n",
    "                                   db_settings.port_number)\r\n",
    "worcking_bd = mongo_client[db_settings.db_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json content:\n",
      "Port Address:                                 localhost\n",
      "Port Number:                                      27017\n",
      "DB Name:                                   TEST_library\n",
      "Replays file:          .\\test_replays\\TestProfilerBatch\n",
      "\n",
      "Local Mongo DB Client\n",
      "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True)\n",
      "\n",
      "Database: \n",
      "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'TEST_library')\n"
     ]
    }
   ],
   "source": [
    "print('config.json content:')\r\n",
    "print(db_settings)\r\n",
    "print('Local Mongo DB Client')\r\n",
    "print(mongo_client, end='\\n\\n')\r\n",
    "print('Database: ')\r\n",
    "print(worcking_bd)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the database is set, I can loop through the replays in a file inserting the return values of the functions from the ingest subpackage's different modules into collections within the database. \n",
    "\n",
    "According to the Ingest and Clustering Sequence Diagram (see Section 1 Intro), I need to define a function that takes a batch of replays and inserts the indexing data into the `replays` collection. The following code shows the use of a loop that would do precisely that. \n",
    "\n",
    "> Note: in the example, I set up a new database called `sample_db` that I will use in this notebook for illustration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 added to replays\n",
      "153 already existed in replays\n"
     ]
    }
   ],
   "source": [
    "sample_db = mongo_client['sample_db']\r\n",
    "collection = sample_db['replays']\r\n",
    "\r\n",
    "replay_batch = sc2reader.load_replays(db_settings.replay_path)\r\n",
    "\r\n",
    "count_add = 0\r\n",
    "count_existed = 0\r\n",
    "for rpl in replay_batch:\r\n",
    "    if not collection.count_documents({'replay_name': rpl.filename}, \r\n",
    "                                      limit = 1):\r\n",
    "        # print(f'Adding {Path(rpl.filename).name} to replays collection.')\r\n",
    "        collection.insert_one(asdict(get_replay_info(rpl)))\r\n",
    "        count_add += 1\r\n",
    "    else:\r\n",
    "        count_existed += 1\r\n",
    "        # print(rpl.filename, \"already exists in the replay_info collection.\")\r\n",
    "\r\n",
    "print(f\"{count_add} added to replays\")\r\n",
    "print(f\"{count_existed} already existed in replays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the loop finishes, the collection has been created in the database. The following snippet shows that this collection is now composed of several documents containing the indexing data for each replay. Additionally, I added a conditional statement within the loop to check if the collection already contains a replay. In that case, the loop will print a warning and avoid inserting repeated documents into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collection now contains: 153 documents\n"
     ]
    }
   ],
   "source": [
    "print('The collection now contains:', \r\n",
    "      collection.estimated_document_count(), 'documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the `indicators` collection\n",
    "\n",
    "Similarly, I can loop through every replay in a batch, extracting the performance indicators for each replays' players. Moreover, I can store these indicators as separate documents in a second collection (i.e. rpl_indicators). This two collection approach was defined in Section 1's introduction.\n",
    "\n",
    "In the following code, I illustrate how such a loop would apply all the functions defined in the `ingest` subpackage to build the indicators of two players in a sample replay. Each player's indicators are stored in a single flat dictionary, i.e. a dictionary that has no nested data structures as values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\r\n",
    "def flatten_indicators(nested_value: dict) -> dict[str, float]:\r\n",
    "    \r\n",
    "    if isinstance(list(nested_value.values())[0], dict):\r\n",
    "        output_dict = {}\r\n",
    "        for k, nested in nested_value.items():\r\n",
    "            for nested_k, v in nested.items():\r\n",
    "                output_dict[f'{k}_{nested_k}'] = v\r\n",
    "        return output_dict\r\n",
    "    elif isinstance(list(nested_value.values())[0], tuple):\r\n",
    "        output_dict = {}\r\n",
    "        order = ['first', 'second']\r\n",
    "        for k, nested in nested_value.items():\r\n",
    "            for ord, v in zip(order, nested):\r\n",
    "                output_dict[f'{ord}_{k}'] = str(v)\r\n",
    "        return output_dict\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: test_replays\\Jagannatha LE.SC2Replay\n",
      "Processing player: pid:1 Player 1 - HDEspino (Protoss)\n",
      "Processing player: pid:2 Player 2 - MxChrisxM (Terran)\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "indicators = sample_db['indicators']\r\n",
    "\r\n",
    "sample_replay = [sc2reader.load_replay('test_replays\\Jagannatha LE.SC2Replay')]\r\n",
    "\r\n",
    "simple_functions = [get_player_macro_econ_stats,\r\n",
    "                    get_expan_times,\r\n",
    "                    get_expan_counts,\r\n",
    "                    calc_attack_ratio,\r\n",
    "                    calc_ctrlg_ratio,\r\n",
    "                    count_max_active_groups,\r\n",
    "                    calc_get_ctrl_grp_ratio,\r\n",
    "                    calc_select_ratio,\r\n",
    "                    list_player_upgrades,\r\n",
    "                    calc_spe_abil_ratios]\r\n",
    "\r\n",
    "double_functions = [count_composition,\r\n",
    "                    count_started]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "indicators_list = []\r\n",
    "for rpl in sample_replay:\r\n",
    "    print(f'Processing: {rpl.filename}')\r\n",
    "    for pid, player in rpl.player.items():\r\n",
    "        print(f'Processing player: pid:{pid} {player}')\r\n",
    "        rpl_indicators = {}\r\n",
    "        \r\n",
    "        for func in simple_functions:\r\n",
    "            rpl_indicators.update(func(rpl, pid))\r\n",
    "\r\n",
    "        for func in double_functions:\r\n",
    "            for flag in [True, False]:\r\n",
    "                rpl_indicators.update(flatten_indicators(func(rpl, pid, flag)))\r\n",
    "                \r\n",
    "        v = get_prefered_spec_abil(rpl, pid)\r\n",
    "        rpl_indicators.update(flatten_indicators(v))\r\n",
    "                \r\n",
    "        indicators_list.append(rpl_indicators)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the code above, `indicaors_list` now contains two dictionaries that store the indicators for each player. \n",
    "\n",
    "> Tip: In some cases, each player's indicators lists would contain a different number of indicators. This difference exists because the list stores separate counts for the players' units and buildings according to their race during the match. Remember that each one of the game races has a different number of available units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of players evaluated:  2\n",
      "First set of indicators belongs to: HDEspino\n",
      "First set of indicators contains:  388 indicators.\n",
      "Second set of indicators belongs to: MxChrisxM\n",
      "Second set of indicators contains:  377 indicators.\n"
     ]
    }
   ],
   "source": [
    "print('Number of players evaluated: ',len(indicators_list))\r\n",
    "print('First set of indicators belongs to:', \r\n",
    "      indicators_list[0]['player_username'])\r\n",
    "print('First set of indicators contains: ',\r\n",
    "        len(indicators_list[0]), 'indicators.')\r\n",
    "print('Second set of indicators belongs to:', \r\n",
    "      indicators_list[1]['player_username'])\r\n",
    "print('Second set of indicators contains: ',\r\n",
    "        len(indicators_list[1]), 'indicators.')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportable functions \r\n",
    "This section defines `inventory_replays`. With this function, users can provide a directory containing multiple `.SC2Replay` files. The function will build four collections in the database specified in the `config.json` file. \r\n",
    "\r\n",
    "Thus, I have condensed two initially proposed functions in the ingestion process diagram, i.e. `inventory_replays` and `get_replay_indicators`, to run in one loop. I also split the indicators by play race into separate collections. This separation will make it easier to compare and condense these replays into the players' race profiles.\r\n",
    "\r\n",
    "Internally, the function uses three helper functions:\r\n",
    "\r\n",
    "- `set_up_db`: connects to the MongoDB client and loads the working database.\r\n",
    "- `verify_replays_path`: makes sure that the path past by the user is valid.\r\n",
    "- `build_indicators`: runs through the indicator extraction loop and stores the results in the indicators collections.\r\n",
    "\r\n",
    "Of these, I export the `set_up_db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def set_up_db():\r\n",
    "    \"\"\"Loads the database specified in the project's config.json file.\r\n",
    "    \"\"\"\r\n",
    "    db_settings = load_configurations()\r\n",
    "    mongo_client = pymongo.MongoClient(db_settings.port_address, \r\n",
    "                                    db_settings.port_number)\r\n",
    "    worcking_bd = mongo_client[db_settings.db_name]\r\n",
    "\r\n",
    "    return worcking_bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\r\n",
    "def verify_replays_path(rpl_path: Any) -> Path:\r\n",
    "\r\n",
    "    if isinstance(rpl_path, str):\r\n",
    "        path = Path(rpl_path)\r\n",
    "    elif not isinstance(rpl_path, Path):\r\n",
    "        string = 'replay_path must be of type str or Path, not ' \r\n",
    "        raise TypeError((string + str(type(replay_batch))))\r\n",
    "    else:\r\n",
    "        path = rpl_path\r\n",
    "\r\n",
    "    if not path.exists():\r\n",
    "        raise ValueError(f'{path} is not valid location')\r\n",
    "\r\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\r\n",
    "def build_indicators(rpl: sc2reader.resources.Replay,\r\n",
    "                    working_db: pymongo.database.Database) -> None:\r\n",
    "    \r\n",
    "    \"\"\"Runs through the indicator extraction loop and stores the results \r\n",
    "    in the indicators collections.\r\n",
    "    \"\"\"\r\n",
    "    simple_functions = [get_player_macro_econ_stats,\r\n",
    "                        get_expan_times,\r\n",
    "                        get_expan_counts,\r\n",
    "                        calc_attack_ratio,\r\n",
    "                        calc_ctrlg_ratio,\r\n",
    "                        count_max_active_groups,\r\n",
    "                        calc_get_ctrl_grp_ratio,\r\n",
    "                        calc_select_ratio,\r\n",
    "                        list_player_upgrades,\r\n",
    "                        calc_spe_abil_ratios,\r\n",
    "                        calc_apms\r\n",
    "                        ]\r\n",
    "\r\n",
    "    double_functions = [count_composition,\r\n",
    "                        count_started]\r\n",
    "    indi_collect = working_db['indicators']\r\n",
    "    for pid in rpl.player.keys():  \r\n",
    "        rpl_indicators = {}\r\n",
    "        for func in simple_functions:\r\n",
    "            rpl_indicators.update(func(rpl, pid))\r\n",
    "\r\n",
    "        v = get_prefered_spec_abil(rpl, pid)\r\n",
    "        rpl_indicators.update(flatten_indicators(v))\r\n",
    "\r\n",
    "        for func in double_functions:\r\n",
    "            for flag in [True, False]:\r\n",
    "                rpl_indicators.update(flatten_indicators(func(rpl, pid, flag)))\r\n",
    "\r\n",
    "        rpl_ind = ({k: v \r\n",
    "                    if (not (isinstance(v, np.int64) \r\n",
    "                        or isinstance(v, np.float64)))\r\n",
    "                    else float(v) for k, v in rpl_indicators.items()})\r\n",
    "        \r\n",
    "        indi_collect.insert_one(rpl_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `inventory_replays`\n",
    "So, to summarise, the main exportable function of the ingest subpackage is `inventory_replays`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def inventory_replays(replay_batch: Any) -> None:\r\n",
    "    \"\"\"This function builds two collections within the database \r\n",
    "    specified in the config.json file.\r\n",
    "\r\n",
    "    The replay information will be stored in the database specified in \r\n",
    "    cwd/data/config.json in the following collections:\r\n",
    "    - `replays`\r\n",
    "        Stores the metadata of the replays, can be used for indexing and\r\n",
    "        for finding the other replays.\r\n",
    "    - `inicators`\r\n",
    "        Store the indicators for each performance of every player.\r\n",
    "\r\n",
    "    *Args:*\r\n",
    "        - replay_batch\r\n",
    "            Directory address where the replays to process are located.\r\n",
    "\r\n",
    "    *Return:*\r\n",
    "        -None\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    working_db = set_up_db()\r\n",
    "    rpls_collect = working_db['replays']\r\n",
    "    path = verify_replays_path(replay_batch)\r\n",
    "\r\n",
    "    replays = sc2reader.load_replays(str(path))\r\n",
    "    \r\n",
    "    load_count = 0\r\n",
    "    process_count = 0\r\n",
    "    previous = 0\r\n",
    "    ignored = 0\r\n",
    "    \r\n",
    "    print(f'Inventorying replays at: {path} in database {working_db.name}')\r\n",
    "\r\n",
    "    for rpl in replays:\r\n",
    "        process_count += 1\r\n",
    "        if (not (rpl.type == \"1v1\")):\r\n",
    "            ignored += 1\r\n",
    "            # print(f'{rpl.filename}is not 1v1')\r\n",
    "            continue\r\n",
    "        if not rpls_collect.count_documents({'replay_name': rpl.filename}, \r\n",
    "                                            limit = 1):\r\n",
    "            # print(f'Processing {rpl.filename}')\r\n",
    "            rpls_collect.insert_one(asdict(get_replay_info(rpl)))\r\n",
    "            build_indicators(rpl, working_db)\r\n",
    "            load_count += 1\r\n",
    "        else:\r\n",
    "            previous += 1\r\n",
    "            # print(rpl.filename, \"already exists in the replay_info rpls_collect.\")\r\n",
    "        \r\n",
    "    print(f'Load complete.')\r\n",
    "    print(f'{process_count} files processed')\r\n",
    "    print(f'{load_count} files loaded')\r\n",
    "    print(f'{ignored} files ignored')\r\n",
    "    print(f'{previous} files alredy existed')\r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the function once I get the following sample results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventorying replays at: c:\\Users\\david\\Documents\\phdcode\\sc_training\\test_replays\\TestProfilerBatch in database TEST_library\n",
      "Load complete.\n",
      "153 files processed\n",
      "149 files loaded\n",
      "4 files ignored\n",
      "0 files alredy existed\n"
     ]
    }
   ],
   "source": [
    "# \r\n",
    "mongo_client.drop_database('TEST_library')\r\n",
    "inventory_replays(test_batch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['replays', 'indicators']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections = [col for col in worcking_bd.list_collection_names()]\r\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replays has 149 records.\n",
      "indicators has 298 records.\n"
     ]
    }
   ],
   "source": [
    "for col in collections:\r\n",
    "    print(f'{col} has {worcking_bd[col].estimated_document_count()} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_Comp_model.ipynb.\n",
      "Converted 01_01_ingest_and_clustering.ipynb.\n",
      "Converted 01_summarise_rpl.ipynb.\n",
      "Converted 02_handle_tracker_events.ipynb.\n",
      "Converted 03_macro_econ_parser.ipynb.\n",
      "Converted 04_build_parser.ipynb.\n",
      "Converted 05_handle_command_events.ipynb.\n",
      "Converted 06_selection_parser.ipynb.\n",
      "Converted 07_ingest.ipynb.\n",
      "Converted 08_profiler.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\r\n",
    "from nbdev.export import notebook2script\r\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sctraining_env': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
